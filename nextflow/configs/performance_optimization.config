/*
 * Performance Optimization Configuration
 * Dynamic resource scaling and intelligent parallelization settings
 */

// Import performance optimization library
import nextflow.lib.PerformanceOptimizer

// Performance optimization parameters
params {
    // Enable performance optimization features
    enable_performance_optimization = true
    enable_dynamic_scaling = true
    enable_intelligent_parallelization = true
    enable_performance_monitoring = true
    
    // Dynamic scaling parameters
    scaling_strategy = 'adaptive'  // Options: 'adaptive', 'conservative', 'aggressive'
    input_size_threshold_gb = 10   // Threshold for triggering size-based scaling
    sample_count_threshold = 5     // Threshold for triggering count-based scaling
    
    // Parallelization parameters
    intelligent_fork_calculation = true
    max_parallel_samples = 20
    batch_processing_enabled = true
    
    // Performance monitoring parameters
    collect_performance_stats = true
    generate_optimization_reports = true
    performance_profiling_level = 'standard'  // Options: 'minimal', 'standard', 'detailed'
    
    // Resource scaling factors
    scaling_factors = [
        memory_scaling_factor: 1.5,
        cpu_scaling_factor: 1.2,
        time_scaling_factor: 1.3,
        io_scaling_factor: 1.1
    ]
    
    // Process-specific optimization settings
    process_optimization = [
        'process_memory_intensive': [
            enable_memory_scaling: true,
            memory_multiplier: 2.0,
            enable_swap_monitoring: true
        ],
        'process_high': [
            enable_cpu_scaling: true,
            cpu_multiplier: 1.5,
            enable_load_balancing: true
        ],
        'process_gpu': [
            enable_gpu_monitoring: true,
            gpu_memory_threshold: 0.8
        ],
        'process_quick': [
            enable_batch_processing: true,
            batch_size_multiplier: 2.0
        ]
    ]
}

// Dynamic resource allocation based on input characteristics
process {
    // Enable performance monitoring for all processes
    beforeScript = params.enable_performance_monitoring ? 
        'echo "Starting performance monitoring for ${task.process} at $(date +%s)" > .performance_start' : ''
    
    afterScript = params.enable_performance_monitoring ? '''
        if [ -f .performance_start ]; then
            start_time=$(cat .performance_start | awk '{print $NF}')
            end_time=$(date +%s)
            duration=$((end_time - start_time))
            echo "Process ${task.process} completed in ${duration} seconds"
            
            # Collect resource usage statistics
            if command -v ps >/dev/null 2>&1; then
                ps -o pid,ppid,cmd,%mem,%cpu --sort=-%mem | head -5 > .resource_usage
            fi
            
            # Log performance metrics
            echo "{\"process\": \"${task.process}\", \"duration\": ${duration}, \"start_time\": ${start_time}, \"end_time\": ${end_time}}" > .performance_metrics.json
        fi
    ''' : ''
    
    // Dynamic resource scaling based on process labels
    withLabel: 'process_low' {
        cpus = { 
            def baseCpus = 2
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalCpus(inputSize, sampleCount, 'process_low', baseCpus)
            }
            return baseCpus * task.attempt
        }
        memory = { 
            def baseMemory = '4.GB'
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalMemory(inputSize, sampleCount, 'process_low', baseMemory)
            }
            return baseMemory.toMemory() * task.attempt
        }
        time = { 
            def baseTime = '2.h'
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalTime(inputSize, sampleCount, 'process_low', baseTime)
            }
            return baseTime.toDuration() * task.attempt
        }
    }
    
    withLabel: 'process_medium' {
        cpus = { 
            def baseCpus = 4
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalCpus(inputSize, sampleCount, 'process_medium', baseCpus)
            }
            return baseCpus * task.attempt
        }
        memory = { 
            def baseMemory = '8.GB'
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalMemory(inputSize, sampleCount, 'process_medium', baseMemory)
            }
            return baseMemory.toMemory() * task.attempt
        }
        time = { 
            def baseTime = '4.h'
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalTime(inputSize, sampleCount, 'process_medium', baseTime)
            }
            return baseTime.toDuration() * task.attempt
        }
    }
    
    withLabel: 'process_high' {
        cpus = { 
            def baseCpus = 8
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalCpus(inputSize, sampleCount, 'process_high', baseCpus)
            }
            return baseCpus * task.attempt
        }
        memory = { 
            def baseMemory = '16.GB'
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalMemory(inputSize, sampleCount, 'process_high', baseMemory)
            }
            return baseMemory.toMemory() * task.attempt
        }
        time = { 
            def baseTime = '8.h'
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalTime(inputSize, sampleCount, 'process_high', baseTime)
            }
            return baseTime.toDuration() * task.attempt
        }
    }
    
    withLabel: 'process_memory_intensive' {
        cpus = { 
            def baseCpus = 4
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalCpus(inputSize, sampleCount, 'process_memory_intensive', baseCpus)
            }
            return baseCpus * task.attempt
        }
        memory = { 
            def baseMemory = '32.GB'
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                def optimizedMemory = PerformanceOptimizer.calculateOptimalMemory(inputSize, sampleCount, 'process_memory_intensive', baseMemory)
                // Apply memory-intensive multiplier
                def multiplier = params.process_optimization?.process_memory_intensive?.memory_multiplier ?: 2.0
                return (optimizedMemory.toMemory() * multiplier).toString()
            }
            return baseMemory.toMemory() * task.attempt
        }
        time = { 
            def baseTime = '12.h'
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalTime(inputSize, sampleCount, 'process_memory_intensive', baseTime)
            }
            return baseTime.toDuration() * task.attempt
        }
        
        // Enable memory monitoring for memory-intensive processes
        ext.enableMemoryMonitoring = params.process_optimization?.process_memory_intensive?.enable_memory_scaling ?: true
    }
    
    withLabel: 'process_gpu' {
        cpus = { 
            def baseCpus = 4
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalCpus(inputSize, sampleCount, 'process_gpu', baseCpus)
            }
            return baseCpus * task.attempt
        }
        memory = { 
            def baseMemory = '16.GB'
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalMemory(inputSize, sampleCount, 'process_gpu', baseMemory)
            }
            return baseMemory.toMemory() * task.attempt
        }
        time = { 
            def baseTime = '8.h'
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalTime(inputSize, sampleCount, 'process_gpu', baseTime)
            }
            return baseTime.toDuration() * task.attempt
        }
        accelerator = 1
        
        // Enable GPU monitoring
        ext.enableGpuMonitoring = params.process_optimization?.process_gpu?.enable_gpu_monitoring ?: true
    }
    
    withLabel: 'process_quick' {
        cpus = { 
            def baseCpus = 1
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalCpus(inputSize, sampleCount, 'process_quick', baseCpus)
            }
            return baseCpus * task.attempt
        }
        memory = { 
            def baseMemory = '2.GB'
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalMemory(inputSize, sampleCount, 'process_quick', baseMemory)
            }
            return baseMemory.toMemory() * task.attempt
        }
        time = { 
            def baseTime = '30.m'
            if (params.enable_dynamic_scaling) {
                def inputSize = task.ext.inputSize ?: 0
                def sampleCount = task.ext.sampleCount ?: 1
                return PerformanceOptimizer.calculateOptimalTime(inputSize, sampleCount, 'process_quick', baseTime)
            }
            return baseTime.toDuration() * task.attempt
        }
        
        // Enable batch processing for quick processes
        ext.enableBatchProcessing = params.process_optimization?.process_quick?.enable_batch_processing ?: true
    }
}

// Intelligent parallelization settings
executor {
    // Dynamic fork calculation based on input characteristics
    if (params.enable_intelligent_parallelization) {
        // This would be calculated dynamically based on input
        queueSize = { 
            def inputFiles = workflow.inputFiles ?: []
            def sampleCount = inputFiles.size()
            def parallelization = PerformanceOptimizer.calculateOptimalParallelization(inputFiles, 'default', params.max_forks ?: 10)
            return parallelization.optimalForks
        }
    }
}

// Performance monitoring configuration
if (params.enable_performance_monitoring) {
    // Enable trace and timeline reports
    trace {
        enabled = true
        file = "${params.outdir}/performance/trace.txt"
        fields = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes'
    }
    
    timeline {
        enabled = true
        file = "${params.outdir}/performance/timeline.html"
    }
    
    report {
        enabled = true
        file = "${params.outdir}/performance/report.html"
    }
    
    dag {
        enabled = true
        file = "${params.outdir}/performance/dag.svg"
    }
}

// Resource optimization profiles
profiles {
    performance_optimized {
        params {
            enable_performance_optimization = true
            enable_dynamic_scaling = true
            enable_intelligent_parallelization = true
            scaling_strategy = 'adaptive'
            performance_profiling_level = 'detailed'
        }
        
        process {
            // Enhanced error handling with performance considerations
            errorStrategy = { 
                if (task.exitStatus in [143,137,104,134,139]) {
                    // Memory or time-related failures - scale up resources
                    return 'retry'
                } else if (task.exitStatus == 130) {
                    // User interruption - don't retry
                    return 'terminate'
                } else {
                    return 'finish'
                }
            }
            
            // Adaptive retry scaling
            maxRetries = { 
                def baseRetries = params.max_retry_scaling ?: 3
                // Reduce retries for quick processes, increase for complex ones
                switch (task.process) {
                    case ~/.*quick.*/:
                        return Math.max(1, baseRetries - 1)
                    case ~/.*memory_intensive.*/:
                        return baseRetries + 1
                    default:
                        return baseRetries
                }
            }
        }
    }
    
    conservative_performance {
        params {
            enable_performance_optimization = true
            enable_dynamic_scaling = false
            scaling_strategy = 'conservative'
            performance_profiling_level = 'standard'
        }
    }
    
    aggressive_performance {
        params {
            enable_performance_optimization = true
            enable_dynamic_scaling = true
            enable_intelligent_parallelization = true
            scaling_strategy = 'aggressive'
            performance_profiling_level = 'detailed'
            
            // More aggressive scaling factors
            scaling_factors = [
                memory_scaling_factor: 2.0,
                cpu_scaling_factor: 1.5,
                time_scaling_factor: 1.5,
                io_scaling_factor: 1.3
            ]
        }
    }
}

// Helper functions for dynamic resource calculation
def calculateInputCharacteristics(inputFiles) {
    def characteristics = [:]
    
    if (inputFiles) {
        def totalSize = 0
        def fileCount = 0
        
        inputFiles.each { file ->
            if (file.exists()) {
                totalSize += file.size()
                fileCount++
            }
        }
        
        characteristics.totalSize = totalSize
        characteristics.fileCount = fileCount
        characteristics.avgFileSize = fileCount > 0 ? totalSize / fileCount : 0
        characteristics.sizeCategory = categorizeInputSize(totalSize)
    }
    
    return characteristics
}

def categorizeInputSize(totalSize) {
    def sizeGB = totalSize / (1024 * 1024 * 1024)
    
    if (sizeGB < 1) return 'small'
    else if (sizeGB < 10) return 'medium'
    else if (sizeGB < 100) return 'large'
    else return 'very_large'
}

// Performance optimization workflow integration
workflow.onComplete {
    if (params.enable_performance_monitoring && params.generate_optimization_reports) {
        // Generate performance optimization report
        def performanceDir = file("${params.outdir}/performance")
        if (!performanceDir.exists()) {
            performanceDir.mkdirs()
        }
        
        // This would trigger the optimization report generation
        log.info "Performance optimization analysis will be available in: ${performanceDir}"
    }
}